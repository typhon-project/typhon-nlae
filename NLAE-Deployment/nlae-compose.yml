version: "3"

services:

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1
    ports:
      - "9200"
    volumes:
      - esdata1:/usr/share/elasticsearch/data

  rabbitmq:
    image: rabbitmq:3-management-alpine
    volumes:
      - ./rabbitmq-definitions.json:/etc/rabbitmq/definitions.json
      - rabbitmq_data:/data
    environment:
        RABBITMQ_ERLANG_COOKIE: tYphoNQISEgd3N9Uowv4
        RABBITMQ_DEFAULT_USER: typhon
        RABBITMQ_DEFAULT_PASS: typhon
    ports:
      - "5672"
      - "15672"

  rest-api:
    image: ehudev/nlae-restapi:latest
    depends_on:
      - elasticsearch
      - rabbitmq
    ports:
      - "8080"

  jobmanager:
    image: ehudev/flink-typhon-nlae:latest
    volumes:
      - ./conf:/opt/flink/conf 
      - /path/to/models:/opt/flink/plugins
    depends_on:
      - elasticsearch
      - rabbitmq
      - rest-api
    ports:
      - "8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      # jobmanager.heap.size = RAM available on Manager nodes (minimum 4GB, recommended 10GB)
      # taskmanager.numberOfTaskSlots = The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.
      - "FLINK_PROPERTIES=jobmanager.heap.size: 10024m \n taskmanager.numberOfTaskSlots: 8"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.role == manager]

  taskmanager:
    image: flink:1.9.0-scala_2.11
    volumes:
      - /path/to/models:/opt/flink/plugins
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      # taskmanager.heap.size = RAM available on Worker nodes (minimum 4GB, recommended 10GB)
      # parallelism.default = The parallelism used for programs that did not specify and other parallelism.
      - "FLINK_PROPERTIES=taskmanager.heap.size: 16024m \n parallelism.default: 8"
    # service deployment
    deploy:
      mode: replicated
      replicas: 2
      # service restart policy
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 60s
      # placement constraint - in this case on 'worker' nodes only
      placement:
        constraints: [node.role == worker]

volumes:
  esdata1:
  rabbitmq_data:
    driver: local

